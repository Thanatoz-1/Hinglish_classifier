{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import keras \n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pprint import pprint\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78244\n"
     ]
    }
   ],
   "source": [
    "data=[]\n",
    "labels=[]\n",
    "total_data=[]\n",
    "\n",
    "# Reading the english doc file\n",
    "with open('./data/english.txt') as f:\n",
    "    data = f.readlines()\n",
    "    labele=np.zeros(len(data), dtype='int8')\n",
    "\n",
    "    # Reading the hinglish doc file\n",
    "with open('./data/hinglish.txt') as f:\n",
    "    data2 = f.readlines()\n",
    "    data+=data2\n",
    "    labelh=np.ones(len(data2), dtype='int8')\n",
    "\n",
    "labels=np.concatenate((labele, labelh), axis=0)\n",
    "    \n",
    "# Cleaning out the newline character from the data\n",
    "for i, word in enumerate(data):\n",
    "    total_data.append((re.sub(\"[^a-zA-Z]\",\"\",word).lower(),labels[i]))\n",
    "print(len(total_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ashirwaad', 1)\n",
      "('thiopental', 0)\n",
      "('hauberget', 0)\n",
      "('tunca', 0)\n",
      "('unsagaciously', 0)\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    pprint(total_data[np.random.randint(len(data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('greisen', 0),\n",
      " ('himalaya', 1),\n",
      " ('thasabhaaon', 1),\n",
      " ('vellum', 0),\n",
      " ('nirwaah', 1),\n",
      " ('farcers', 0),\n",
      " ('bistros', 0),\n",
      " ('cytodieretic', 0),\n",
      " ('foliosity', 0),\n",
      " ('baarah', 1)]\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle(total_data)\n",
    "test = list(total_data[:500])\n",
    "data = list(total_data[500:])\n",
    "pprint(test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [a[0] for a in data]\n",
    "train_y = [a[1] for a in data]\n",
    "test_x = [a[0] for a in test]\n",
    "test_y = [a[1] for a in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The character set is: \n",
      " ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Maximum length word:  gatimayatageyatasangitamayata\n",
      "Number of characters in longest word  29\n",
      "Words in the total dataset are 78244 and in train_x are 77744\n"
     ]
    }
   ],
   "source": [
    "char_set = sorted(set(''.join(train_x)))\n",
    "print(\"The character set is: \\n\",char_set)\n",
    "longest = max(sorted(test_x, key=len)[-1],sorted(train_x, key=len)[-1])\n",
    "print(\"Maximum length word: \",longest)\n",
    "maxlen = len(longest)\n",
    "print(\"Number of characters in longest word \",maxlen)\n",
    "word_count = len(train_x)\n",
    "print(\"Words in the total dataset are %s and in train_x are %s\"%(len(total_data), word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpToDig=dict()\n",
    "digToAlp=dict()\n",
    "for i,j in enumerate(char_set):\n",
    "    alpToDig[j]=i+1\n",
    "    digToAlp[i+1]=j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(word):\n",
    "    processed_frame_x=np.zeros((len(word),maxlen,len(char_set)),dtype='int8')\n",
    "    for sample_index,sample in enumerate(word):\n",
    "        for char_index, char in enumerate(sample.lower()):\n",
    "            processed_frame_x[sample_index, char_index-1, alpToDig[char]-1]=1\n",
    "    return processed_frame_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frame_y = np.array(train_y)\n",
    "test_frame_y = np.array(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frame_x = process_text(train_x)\n",
    "test_frame_x = process_text(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.LSTM(8, input_shape=(maxlen, len(char_set))))\n",
    "model.add(keras.layers.Dense(1))\n",
    "model.add(keras.layers.Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile( optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "77744/77744 [==============================] - 73s 938us/step - loss: 0.3667 - acc: 0.8390\n",
      "Epoch 2/5\n",
      "77744/77744 [==============================] - 72s 925us/step - loss: 0.2787 - acc: 0.8842\n",
      "Epoch 3/5\n",
      "77744/77744 [==============================] - 62s 796us/step - loss: 0.2687 - acc: 0.8893\n",
      "Epoch 4/5\n",
      "77744/77744 [==============================] - 60s 771us/step - loss: 0.2606 - acc: 0.8927\n",
      "Epoch 5/5\n",
      "77744/77744 [==============================] - 60s 774us/step - loss: 0.2510 - acc: 0.8977\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_frame_x, train_frame_y,\n",
    "                    batch_size=64,\n",
    "                     epochs=5,\n",
    "                       verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_frame_x, test_frame_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['Chalo','coffee','peete','hai']\n",
    "processed_words = process_text(words)\n",
    "pred = model.predict_classes(processed_words)\n",
    "for i,p in enumerate(pred):\n",
    "    if(pred[i]==0):\n",
    "        print('%s is English'%words[i])\n",
    "    else:\n",
    "        print('%s is Hinglish'%words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('hinglish_classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (keras)",
   "language": "python",
   "name": "kerass"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
