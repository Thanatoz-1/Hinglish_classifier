{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import keras \n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pprint import pprint\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78244\n"
     ]
    }
   ],
   "source": [
    "data=[]\n",
    "labels=[]\n",
    "total_data=[]\n",
    "\n",
    "# Reading the english doc file\n",
    "with open('./data/english.txt') as f:\n",
    "    data = f.readlines()\n",
    "    labele=np.zeros(len(data), dtype='int8')\n",
    "\n",
    "    # Reading the hinglish doc file\n",
    "with open('./data/hinglish.txt') as f:\n",
    "    data2 = f.readlines()\n",
    "    data+=data2\n",
    "    labelh=np.ones(len(data2), dtype='int8')\n",
    "\n",
    "labels=np.concatenate((labele, labelh), axis=0)\n",
    "    \n",
    "# Cleaning out the newline character from the data\n",
    "for i, word in enumerate(data):\n",
    "    total_data.append((re.sub(\"[^a-zA-Z]\",\"\",word).lower(),labels[i]))\n",
    "print(len(total_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ande', 1)\n",
      "('praefects', 0)\n",
      "('pectinatodenticulate', 0)\n",
      "('phantasiast', 0)\n",
      "('immoralizing', 0)\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    pprint(total_data[np.random.randint(len(data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('yahaa', 1),\n",
      " ('ubaoo', 1),\n",
      " ('haibade', 1),\n",
      " ('ber', 1),\n",
      " ('undercrossing', 0),\n",
      " ('bharabhitapranan', 1),\n",
      " ('soldo', 0),\n",
      " ('uchakaye', 1),\n",
      " ('counterraising', 0),\n",
      " ('srijak', 1)]\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle(total_data)\n",
    "test = list(total_data[:500])\n",
    "data = list(total_data[500:])\n",
    "pprint(test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [a[0] for a in data]\n",
    "train_y = [a[1] for a in data]\n",
    "test_x = [a[0] for a in test]\n",
    "test_y = [a[1] for a in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The character set is: \n",
      " ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Maximum length word:  trinitrophenylmethylnitramine\n",
      "Number of characters in longest word  29\n",
      "Words in the total dataset are 78244 and in train_x are 77744\n"
     ]
    }
   ],
   "source": [
    "char_set = sorted(set(''.join(train_x)))\n",
    "print(\"The character set is: \\n\",char_set)\n",
    "longest = max(sorted(test_x, key=len)[-1],sorted(train_x, key=len)[-1])\n",
    "print(\"Maximum length word: \",longest)\n",
    "maxlen = len(longest)\n",
    "print(\"Number of characters in longest word \",maxlen)\n",
    "word_count = len(train_x)\n",
    "print(\"Words in the total dataset are %s and in train_x are %s\"%(len(total_data), word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpToDig=dict()\n",
    "digToAlp=dict()\n",
    "for i,j in enumerate(char_set):\n",
    "    alpToDig[j]=i+1\n",
    "    digToAlp[i+1]=j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(word):\n",
    "    processed_frame_x=np.zeros((len(word),maxlen,len(char_set)),dtype='int8')\n",
    "    for sample_index,sample in enumerate(word):\n",
    "        for char_index, char in enumerate(sample.lower()):\n",
    "            processed_frame_x[sample_index, char_index-1, alpToDig[char]-1]=1\n",
    "    return processed_frame_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frame_y = np.array(train_y)\n",
    "test_frame_y = np.array(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frame_x = process_text(train_x)\n",
    "test_frame_x = process_text(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.LSTM(8, input_shape=(maxlen, len(char_set))))\n",
    "model.add(keras.layers.Dense(1))\n",
    "model.add(keras.layers.Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile( optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "77744/77744 [==============================] - 71s 907us/step - loss: 0.3522 - acc: 0.8451\n",
      "Epoch 2/4\n",
      "77744/77744 [==============================] - 76s 982us/step - loss: 0.2737 - acc: 0.8867\n",
      "Epoch 3/4\n",
      "77744/77744 [==============================] - 77s 986us/step - loss: 0.2604 - acc: 0.8926\n",
      "Epoch 4/4\n",
      "77744/77744 [==============================] - 79s 1ms/step - loss: 0.2499 - acc: 0.8979\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_frame_x, train_frame_y,\n",
    "                    batch_size=64,\n",
    "                     epochs=4,\n",
    "                       verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.22405415862798692, 0.918]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_frame_x, test_frame_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coffee is English\n",
      "pi is Hinglish\n",
      "lo is Hinglish\n",
      "friends is English\n"
     ]
    }
   ],
   "source": [
    "words = ['Coffee','pi','lo','friends']\n",
    "processed_words = process_text(words)\n",
    "pred = model.predict_classes(processed_words)\n",
    "for i,p in enumerate(pred):\n",
    "    if(pred[i]==0):\n",
    "        print('%s is English'%words[i])\n",
    "    else:\n",
    "        print('%s is Hinglish'%words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (keras)",
   "language": "python",
   "name": "kerass"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
